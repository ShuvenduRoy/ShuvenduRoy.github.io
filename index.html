<!DOCTYPE html>
<html class=" js " lang="en">
<head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta charset="utf-8"> <!-- begin SEO --><title>Shuvendu Roy</title>
    <meta property="og:locale" content="en-US">
    <meta property="og:site_name" content="Shuvendu Roy">
    <meta property="og:title" content="Shuvendu Roy">
    <link rel="canonical" href="https://shuvenduroy.github.io/">
    <meta property="og:url" content="https://shuvenduroy.github.io/">
    <meta property="og:description" content="About me">
    <script async="" src="backend/analytics.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=The+Nautigal:wght@700&display=swap" rel="stylesheet">
    <script type="application/ld+json"> {
        "@context": "http://schema.org",
        "@type": "Person",
        "name": "Your Name",
        "url": "https://shuvenduroy.github.io",
        "sameAs": null
    } </script> <!-- end SEO -->
    <link href="css/font_awesome.css" rel="stylesheet">
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script> document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js '; </script> <!-- For all browsers -->
    <link rel="stylesheet" href="backend/main.css">
    <meta name="theme-color" content="#ffffff">
    <link rel="stylesheet" href="backend/academicons.css">
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="css/main.css">
    <script type="text/x-mathjax-config;executed=true"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
    <script type="text/x-mathjax-config;executed=true"> MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } }); </script>
    <script src="backend/latest.js" async=""></script> <!-- end custom head snippets -->
    <style id="fit-vids-style">.fluid-width-video-wrapper {
        width: 100%;
        position: relative;
        padding: 0;
    }

    .fluid-width-video-wrapper iframe, .fluid-width-video-wrapper object, .fluid-width-video-wrapper embed {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
    }</style>
    <script type="text/javascript" async="" src="backend/MathJax.js"></script>
    <style type="text/css">.MathJax_Hover_Frame {
        border-radius: .25em;
        -webkit-border-radius: .25em;
        -moz-border-radius: .25em;
        -khtml-border-radius: .25em;
        box-shadow: 0px 0px 15px #83A;
        -webkit-box-shadow: 0px 0px 15px #83A;
        -moz-box-shadow: 0px 0px 15px #83A;
        -khtml-box-shadow: 0px 0px 15px #83A;
        border: 1px solid #A6D ! important;
        display: inline-block;
        position: absolute
    }

    .MathJax_Menu_Button .MathJax_Hover_Arrow {
        position: absolute;
        cursor: pointer;
        display: inline-block;
        border: 2px solid #AAA;
        border-radius: 4px;
        -webkit-border-radius: 4px;
        -moz-border-radius: 4px;
        -khtml-border-radius: 4px;
        font-family: 'Courier New', Courier;
        font-size: 9px;
        color: #F0F0F0
    }

    .MathJax_Menu_Button .MathJax_Hover_Arrow span {
        display: block;
        background-color: #AAA;
        border: 1px solid;
        border-radius: 3px;
        line-height: 0;
        padding: 4px
    }

    .MathJax_Hover_Arrow:hover {
        color: white !important;
        border: 2px solid #CCC !important
    }

    .MathJax_Hover_Arrow:hover span {
        background-color: #CCC !important
    }
    </style>
    <style type="text/css">#MathJax_About {
        position: fixed;
        left: 50%;
        width: auto;
        text-align: center;
        border: 3px outset;
        padding: 1em 2em;
        background-color: #DDDDDD;
        color: black;
        cursor: default;
        font-family: message-box;
        font-size: 120%;
        font-style: normal;
        text-indent: 0;
        text-transform: none;
        line-height: normal;
        letter-spacing: normal;
        word-spacing: normal;
        word-wrap: normal;
        white-space: nowrap;
        float: none;
        z-index: 201;
        border-radius: 15px;
        -webkit-border-radius: 15px;
        -moz-border-radius: 15px;
        -khtml-border-radius: 15px;
        box-shadow: 0px 10px 20px #808080;
        -webkit-box-shadow: 0px 10px 20px #808080;
        -moz-box-shadow: 0px 10px 20px #808080;
        -khtml-box-shadow: 0px 10px 20px #808080;
        filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')
    }

    #MathJax_About.MathJax_MousePost {
        outline: none
    }

    .MathJax_Menu {
        position: absolute;
        background-color: white;
        color: black;
        width: auto;
        padding: 2px;
        border: 1px solid #CCCCCC;
        margin: 0;
        cursor: default;
        font: menu;
        text-align: left;
        text-indent: 0;
        text-transform: none;
        line-height: normal;
        letter-spacing: normal;
        word-spacing: normal;
        word-wrap: normal;
        white-space: nowrap;
        float: none;
        z-index: 201;
        box-shadow: 0px 10px 20px #808080;
        -webkit-box-shadow: 0px 10px 20px #808080;
        -moz-box-shadow: 0px 10px 20px #808080;
        -khtml-box-shadow: 0px 10px 20px #808080;
        filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')
    }

    .MathJax_MenuItem {
        padding: 2px 2em;
        background: transparent
    }

    .MathJax_MenuArrow {
        position: absolute;
        right: .5em;
        padding-top: .25em;
        color: #666666;
        font-size: .75em
    }

    .MathJax_MenuActive .MathJax_MenuArrow {
        color: white
    }

    .MathJax_MenuArrow.RTL {
        left: .5em;
        right: auto
    }

    .MathJax_MenuCheck {
        position: absolute;
        left: .7em
    }

    .MathJax_MenuCheck.RTL {
        right: .7em;
        left: auto
    }

    .MathJax_MenuRadioCheck {
        position: absolute;
        left: 1em
    }

    .MathJax_MenuRadioCheck.RTL {
        right: 1em;
        left: auto
    }

    .MathJax_MenuLabel {
        padding: 2px 2em 4px 1.33em;
        font-style: italic
    }

    .MathJax_MenuRule {
        border-top: 1px solid #CCCCCC;
        margin: 4px 1px 0px
    }

    .MathJax_MenuDisabled {
        color: GrayText
    }

    .MathJax_MenuActive {
        background-color: Highlight;
        color: HighlightText
    }

    .MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {
        background-color: #E8E8E8
    }

    .MathJax_ContextMenu:focus {
        outline: none
    }

    .MathJax_ContextMenu .MathJax_MenuItem:focus {
        outline: none
    }

    #MathJax_AboutClose {
        top: .2em;
        right: .2em
    }

    .MathJax_Menu .MathJax_MenuClose {
        top: -10px;
        left: -10px
    }

    .MathJax_MenuClose {
        position: absolute;
        cursor: pointer;
        display: inline-block;
        border: 2px solid #AAA;
        border-radius: 18px;
        -webkit-border-radius: 18px;
        -moz-border-radius: 18px;
        -khtml-border-radius: 18px;
        font-family: 'Courier New', Courier;
        font-size: 24px;
        color: #F0F0F0
    }

    .MathJax_MenuClose span {
        display: block;
        background-color: #AAA;
        border: 1.5px solid;
        border-radius: 18px;
        -webkit-border-radius: 18px;
        -moz-border-radius: 18px;
        -khtml-border-radius: 18px;
        line-height: 0;
        padding: 8px 0 6px
    }

    .MathJax_MenuClose:hover {
        color: white !important;
        border: 2px solid #CCC !important
    }

    .MathJax_MenuClose:hover span {
        background-color: #CCC !important
    }

    .MathJax_MenuClose:hover:focus {
        outline: none
    }
    </style>
    <style type="text/css">.MathJax_Preview .MJXf-math {
        color: inherit !important
    }
    </style>
    <style type="text/css">.MJX_Assistive_MathML {
        position: absolute !important;
        top: 0;
        left: 0;
        clip: rect(1px, 1px, 1px, 1px);
        padding: 1px 0 0 0 !important;
        border: 0 !important;
        height: 1px !important;
        width: 1px !important;
        overflow: hidden !important;
        display: block !important;
        -webkit-touch-callout: none;
        -webkit-user-select: none;
        -khtml-user-select: none;
        -moz-user-select: none;
        -ms-user-select: none;
        user-select: none
    }

    .MJX_Assistive_MathML.MJX_Assistive_MathML_Block {
        width: 100% !important
    }
    </style>
    <style type="text/css">#MathJax_Zoom {
        position: absolute;
        background-color: #F0F0F0;
        overflow: auto;
        display: block;
        z-index: 301;
        padding: .5em;
        border: 1px solid black;
        margin: 0;
        font-weight: normal;
        font-style: normal;
        text-align: left;
        text-indent: 0;
        text-transform: none;
        line-height: normal;
        letter-spacing: normal;
        word-spacing: normal;
        word-wrap: normal;
        white-space: nowrap;
        float: none;
        -webkit-box-sizing: content-box;
        -moz-box-sizing: content-box;
        box-sizing: content-box;
        box-shadow: 5px 5px 15px #AAAAAA;
        -webkit-box-shadow: 5px 5px 15px #AAAAAA;
        -moz-box-shadow: 5px 5px 15px #AAAAAA;
        -khtml-box-shadow: 5px 5px 15px #AAAAAA;
        filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')
    }

    #MathJax_ZoomOverlay {
        position: absolute;
        left: 0;
        top: 0;
        z-index: 300;
        display: inline-block;
        width: 100%;
        height: 100%;
        border: 0;
        padding: 0;
        margin: 0;
        background-color: white;
        opacity: 0;
        filter: alpha(opacity=0)
    }

    #MathJax_ZoomFrame {
        position: relative;
        display: inline-block;
        height: 0;
        width: 0
    }

    #MathJax_ZoomEventTrap {
        position: absolute;
        left: 0;
        top: 0;
        z-index: 302;
        display: inline-block;
        border: 0;
        padding: 0;
        margin: 0;
        background-color: white;
        opacity: 0;
        filter: alpha(opacity=0)
    }
    </style>
    <style type="text/css">.MathJax_Preview {
        color: #888
    }

    #MathJax_Message {
        position: fixed;
        left: 1px;
        bottom: 2px;
        background-color: #E6E6E6;
        border: 1px solid #959595;
        margin: 0px;
        padding: 2px 8px;
        z-index: 102;
        color: black;
        font-size: 80%;
        width: auto;
        white-space: nowrap
    }

    #MathJax_MSIE_Frame {
        position: absolute;
        top: 0;
        left: 0;
        width: 0px;
        z-index: 101;
        border: 0px;
        margin: 0px;
        padding: 0px
    }

    .MathJax_Error {
        color: #CC0000;
        font-style: italic
    }
    </style>
    <style type="text/css">.MJXp-script {
        font-size: .8em
    }

    .MJXp-right {
        -webkit-transform-origin: right;
        -moz-transform-origin: right;
        -ms-transform-origin: right;
        -o-transform-origin: right;
        transform-origin: right
    }

    .MJXp-bold {
        font-weight: bold
    }

    .MJXp-italic {
        font-style: italic
    }

    .MJXp-scr {
        font-family: MathJax_Script, 'Times New Roman', Times, STIXGeneral, serif
    }

    .MJXp-frak {
        font-family: MathJax_Fraktur, 'Times New Roman', Times, STIXGeneral, serif
    }

    .MJXp-sf {
        font-family: MathJax_SansSerif, 'Times New Roman', Times, STIXGeneral, serif
    }

    .MJXp-cal {
        font-family: MathJax_Caligraphic, 'Times New Roman', Times, STIXGeneral, serif
    }

    .MJXp-mono {
        font-family: MathJax_Typewriter, 'Times New Roman', Times, STIXGeneral, serif
    }

    .MJXp-largeop {
        font-size: 150%
    }

    .MJXp-largeop.MJXp-int {
        vertical-align: -.2em
    }

    .MJXp-math {
        display: inline-block;
        line-height: 1.2;
        text-indent: 0;
        font-family: 'Times New Roman', Times, STIXGeneral, serif;
        white-space: nowrap;
        border-collapse: collapse
    }

    .MJXp-display {
        display: block;
        text-align: center;
        margin: 1em 0
    }

    .MJXp-math span {
        display: inline-block
    }

    .MJXp-box {
        display: block !important;
        text-align: center
    }

    .MJXp-box:after {
        content: " "
    }

    .MJXp-rule {
        display: block !important;
        margin-top: .1em
    }

    .MJXp-char {
        display: block !important
    }

    .MJXp-mo {
        margin: 0 .15em
    }

    .MJXp-mfrac {
        margin: 0 .125em;
        vertical-align: .25em
    }

    .MJXp-denom {
        display: inline-table !important;
        width: 100%
    }

    .MJXp-denom > * {
        display: table-row !important
    }

    .MJXp-surd {
        vertical-align: top
    }

    .MJXp-surd > * {
        display: block !important
    }

    .MJXp-script-box > * {
        display: table !important;
        height: 50%
    }

    .MJXp-script-box > * > * {
        display: table-cell !important;
        vertical-align: top
    }

    .MJXp-script-box > *:last-child > * {
        vertical-align: bottom
    }

    .MJXp-script-box > * > * > * {
        display: block !important
    }

    .MJXp-mphantom {
        visibility: hidden
    }

    .MJXp-munderover {
        display: inline-table !important
    }

    .MJXp-over {
        display: inline-block !important;
        text-align: center
    }

    .MJXp-over > * {
        display: block !important
    }

    .MJXp-munderover > * {
        display: table-row !important
    }

    .MJXp-mtable {
        vertical-align: .25em;
        margin: 0 .125em
    }

    .MJXp-mtable > * {
        display: inline-table !important;
        vertical-align: middle
    }

    .MJXp-mtr {
        display: table-row !important
    }

    .MJXp-mtd {
        display: table-cell !important;
        text-align: center;
        padding: .5em 0 0 .5em
    }

    .MJXp-mtr > .MJXp-mtd:first-child {
        padding-left: 0
    }

    .MJXp-mtr:first-child > .MJXp-mtd {
        padding-top: 0
    }

    .MJXp-mlabeledtr {
        display: table-row !important
    }

    .MJXp-mlabeledtr > .MJXp-mtd:first-child {
        padding-left: 0
    }

    .MJXp-mlabeledtr:first-child > .MJXp-mtd {
        padding-top: 0
    }

    .MJXp-merror {
        background-color: #FFFF88;
        color: #CC0000;
        border: 1px solid #CC0000;
        padding: 1px 3px;
        font-style: normal;
        font-size: 90%
    }

    .MJXp-scale0 {
        -webkit-transform: scaleX(.0);
        -moz-transform: scaleX(.0);
        -ms-transform: scaleX(.0);
        -o-transform: scaleX(.0);
        transform: scaleX(.0)
    }

    .MJXp-scale1 {
        -webkit-transform: scaleX(.1);
        -moz-transform: scaleX(.1);
        -ms-transform: scaleX(.1);
        -o-transform: scaleX(.1);
        transform: scaleX(.1)
    }

    .MJXp-scale2 {
        -webkit-transform: scaleX(.2);
        -moz-transform: scaleX(.2);
        -ms-transform: scaleX(.2);
        -o-transform: scaleX(.2);
        transform: scaleX(.2)
    }

    .MJXp-scale3 {
        -webkit-transform: scaleX(.3);
        -moz-transform: scaleX(.3);
        -ms-transform: scaleX(.3);
        -o-transform: scaleX(.3);
        transform: scaleX(.3)
    }

    .MJXp-scale4 {
        -webkit-transform: scaleX(.4);
        -moz-transform: scaleX(.4);
        -ms-transform: scaleX(.4);
        -o-transform: scaleX(.4);
        transform: scaleX(.4)
    }

    .MJXp-scale5 {
        -webkit-transform: scaleX(.5);
        -moz-transform: scaleX(.5);
        -ms-transform: scaleX(.5);
        -o-transform: scaleX(.5);
        transform: scaleX(.5)
    }

    .MJXp-scale6 {
        -webkit-transform: scaleX(.6);
        -moz-transform: scaleX(.6);
        -ms-transform: scaleX(.6);
        -o-transform: scaleX(.6);
        transform: scaleX(.6)
    }

    .MJXp-scale7 {
        -webkit-transform: scaleX(.7);
        -moz-transform: scaleX(.7);
        -ms-transform: scaleX(.7);
        -o-transform: scaleX(.7);
        transform: scaleX(.7)
    }

    .MJXp-scale8 {
        -webkit-transform: scaleX(.8);
        -moz-transform: scaleX(.8);
        -ms-transform: scaleX(.8);
        -o-transform: scaleX(.8);
        transform: scaleX(.8)
    }

    .MJXp-scale9 {
        -webkit-transform: scaleX(.9);
        -moz-transform: scaleX(.9);
        -ms-transform: scaleX(.9);
        -o-transform: scaleX(.9);
        transform: scaleX(.9)
    }

    .MathJax_PHTML .noError {
        font-size: 90%;
        text-align: left;
        color: black;
        padding: 1px 3px;
        border: 1px solid
    }
    </style>
    <script src="backend/jquery-3.6.0.min.js"></script>
    <script>
        $(function () {
            $("#includeTop").load("topbar.html");
        });
        $(function () {
            $("#includeSide").load("sidebar.html");
        });
    </script>

</head>

<body data-new-gr-c-s-check-loaded="8.892.0" data-gr-ext-installed="">
<div id="MathJax_Message" style="display: none;"></div>
<!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your
    experience.
</div><![endif]-->

<div id="includeTop"></div>

<div id="main" role="main">
    <div id="includeSide"></div>
    <h1>
        <span style="color: #000000; font-family: 'The Nautigal', cursive; font-size: 2.0em; ">Shuvendu Roy</span>
    </h1>


    <article class="page" itemscope="" itemtype="">
        <meta itemprop="headline" content="Shuvendu Roy">
        <meta itemprop="description" content="About me">
        <div class="page__inner-wrap">
            <!--            <header><h1 class="page__title" itemprop="Shuvendu Roy">Bio</h1></header>-->
            <section class="page__content" itemprop="text">
<!--                <p align="center" style="color:black">-->
<!--                    Graduate Researcher <br>-->
<!--                    Queen's University, ON, Canada <br>-->
<!--                    shuvendu.roy@queensu.ca <br>-->
<!--                </p>-->
<hr>
                <p align="justify" style="color:black">

                    Hi! I'm Shuvendu Roy, a Ph.D Candidate,
                    at <a href="https://www.aiimlab.com/"> Ambient Intelligence and Interactive Machines Laboratory (Aiim Lab)</a>
                    and <a href="https://ingenuitylabs.queensu.ca/">Ingenuity Labs Research Institute</a>,
                    <a href="https://www.queensu.ca/">Queen’s University</a>, Canada.
                    I am very fortunate to be supervised by <a HREF="https://www.aiimlab.com/director">Prof. Ali Etemad</a>.
                    <!--					My research focus in broadly centered around Computer Vision and Deep learning. -->
                    <!--					In particular, I work on unsupervised learning, e.g. self-supervised and semi-supervised learning. -->
                    My research focuses on Multi-modal Unsupervised Representation Learning and Downstream Adaptation with a focus on reducing the need for large labelled data.
                </p>

                <p align="justify" style="color:black">
                    A summary of my profile is as follows:
                    Published in top tier conference and journals such as ICLR, AAAI, TMLR, with a total of 30+ publications;
                    Received 500+ citations;
                    Skilled in python and different machine learning libraries (Pytorch, Tensorflow, etc);
                    Experienced in deep learning method and model designing, training and deployment;
                    7+ years of involvement in academic and industry research of machine learning methods;
                    Previous job/internship experiences in Google Research, and Vector Institute for AI.
                </p>

                <p align="center">
                    <a href="mailto:shuvendu.roy@queensu.ca"><b>Email</b></a> &nbsp;|&nbsp;
                    <a href="data/CV___Shuvendu_Roy.pdf" target="_blank"><b>CV</b></a> &nbsp;|&nbsp;
                    <a href="https://scholar.google.com/citations?user=5-zu4ZsAAAAJ&hl=en"><b>Google Scholar</b></a> &nbsp;|&nbsp;
                    <a href="https://www.linkedin.com/in/shuvenduroy/"><b>LinkedIn</b></a> &nbsp;|&nbsp;
                    <!--                    <a href="https://www.researchgate.net/profile/Abdul-Muntakim-Rafi/publications">ResearchGate</a> &nbsp;|&nbsp;-->
                    <a href="https://github.com/ShuvenduRoy"><b>Github</b></a>
                    <!--					<a href="data/CV_of_Failure.pdf" target="_blank">CV of Failures</a>-->
                    <!--					&nbsp;|&nbsp;-->
                </p>


                <!--				<h1 id="research_interest">Research Interests</h1>-->
                <!--				<p align="justify"  style="color:black">-->
                <!--					My research focus in broadly centered around Computer Vision and Deep learning. In particular, I work on unsupervised learning, e.g. self-supervised and semi-supervised learning. My research goal is to explore intelligent methods with less reliance on large dataset. My research also focus on the applications of deep learning in affective computing.-->
                <!--				</p>-->
                <hr>

                <br><br>

                <header><h1 class="page__title" itemprop="headline" id="bio"><i>Education</i></h1></header>
<!--                <h2 dir="ltr" class="CDt4Ke zfr3Q JYVBee"><span style="color: #000000; font-family: 'Nunito'; font-size: 1.3em; font-weight: normal; ">Education</span><span-->
<!--                        style="color: #000000; font-family: 'Nunito'; font-size: 14pt; font-weight: normal; vertical-align: baseline;"></span></h2>-->

                <div class="page__inner-wrap" style="padding-left: 0px">
                    <p dir="ltr" style="line-height: 0; text-align: left;">
                        <ul class="three">
                            <li><b>Doctor of Philosophy</b>, Dept. of Electrical and Computer Engineering, Queen's University, Canada, 2022-Present.</li>
                            <li><b>Master of Applied Science</b>, Dept. of Electrical and Computer Engineering, Queen's University, Canada, 2020-2021.</li>
                        </ul>
                    </p>
                    <hr>

                    <br><br>
                </div>




                <header><h1 class="page__title" itemprop="headline"><i>Experiences</i></h1></header>
<!--                <h2 dir="ltr" class="CDt4Ke zfr3Q JYVBee"><span style="color: #000000; font-family: 'Nunito'; font-size: 1.3em; font-weight: normal; ">Experiences</span><span-->
<!--                        style="color: #000000; font-family: 'Nunito'; font-size: 14pt; font-weight: normal; vertical-align: baseline;"></span></h2>-->
                <div class="page__inner-wrap" style="padding-left: 0px">
                    <p dir="ltr" style="line-height: 0; text-align: left;">
                        <ul class="three">
                            <li><b>Student Researcher</b>, Google, May 2023 - Present.</li>
                                <p style="margin-top: -10px; line-height: 1.2;">Currently working as a Student Research at google. The focus of my reseach is self-supervised represeantaion learning under special data settings.</p>
                            <li><b>Research Assistant</b>, Queen's University, Kingston, Canada, 2020 - Present.</li>
                                <p style="margin-top: -10px; line-height: 1.2;">I am currently working as a research assistant at <a href="https://www.aiimlab.com/"> Ambient Intelligence and Interactive Machines Laboratory (Aiim Lab)</a> and <a href="https://ingenuitylabs.queensu.ca/">Ingenuity Labs Research Institute</a>, <a href="https://www.queensu.ca/">Queen’s University</a>, Kingston, ON, Canada. I am supervised by <a HREF="https://www.aiimlab.com/director">Prof. Ali Etemad</a>. My research focus is Computer Vision and Deep Learning with special interest on Unsupervised learning (e.g. Self-supervised learning, and Semi-supervised learning).</p>
                            <li><b>Teaching Assistant</b>, Queen's University, Kingston, Canada, 2020 - Present.</li>
                                <ol>
                                    <li> ELEC 472, Artificial Intelligence, Winter 2022.</li>
                                    <li> APSC 143, Introduction to Programming, Fall 2022.</li>
                                </ol>
                            <li><b>Applied ML Researcher</b>, Robi Axiata Ltd., Bangladesh, 2019-2020.</li>
                            <li><b>Jr. Software Engineer</b>, REVE Systems, Bangladesh, 2019.</li>
                        </ul>
                    </p>
                    <hr>

                    <br><br><br>
                </div>


                <header><h1 class="page__title" itemprop="headline"  ><i>Research</i></h1></header>
                <section class="page__content" itemprop="text" id="research">
                <table style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-right:auto; margin-left:auto; line-height: 1.3">
                    <tbody>


<!--                    <tr onmouseout="aaai24_stop()" onmouseover="aaai24_start()" bgcolor="#ffffff">-->
<!--                        <td style="width:25%; vertical-align:center">-->
<!--                            <div class="one">-->
<!--                                <div class="two" id='aaai24_image'>-->
<!--                                    <img src="img/aaai24_2.png" id="aaai24_img_2" width="200">-->
<!--                                </div>-->
<!--                                <img src='img/aaai24_1.png' id="aaai24_img_1" width="200">-->
<!--                            </div>-->
<!--                            <script type="text/javascript">-->
<!--                                function aaai24_start() {-->
<!--                                    document.getElementById('aaai24_image').style.opacity = "1";-->
<!--                                    document.getElementById('aaai24').style.display = 'block';-->
<!--                                    document.getElementById('aaai24_img_1').style.opacity = "0";-->
<!--                                    document.getElementById('aaai24_tldr').style.display = 'none';-->

<!--                                }-->

<!--                                function aaai24_stop() {-->
<!--                                    document.getElementById('aaai24_image').style.opacity = "0";-->
<!--                                    document.getElementById('aaai24').style.display = "none";-->
<!--                                    document.getElementById('aaai24_img_1').style.opacity = "1";-->
<!--                                    document.getElementById('aaai24_tldr').style.display = 'block';-->

<!--                                }-->

<!--                                aaai24_stop()-->
<!--                            </script>-->
<!--                        </td>-->
<!--                        <td style="width:75% ;vertical-align:top">-->
<!--                            <a href="https://ojs.aaai.org/index.php/AAAI/article/view/29404" style="text-decoration: None">-->
<!--                                <strong>Scaling Up Semi-supervised Learning with Unconstrained Unlabelled Data</strong>-->
<!--                            </a>-->
<!--                            <br><br>-->
<!--                            <strong>Shuvendu Roy</strong>, Ali Etemad-->
<!--                            <br>-->
<!--                            AAAI Conference on Artificial Intelligence (<b>AAAI 2023</b>) <br>-->
<!--                            <br>-->

<!--                            <a style="text-decoration: None" href="https://ojs.aaai.org/index.php/AAAI/article/view/29404" target="_blank">Paper</a> |-->
<!--                            <a style="text-decoration: None" href="https://arxiv.org/pdf/2306.01222" target="_blank">ArXiv</a> |-->
<!--                            <a style="text-decoration: None" href="https://github.com/ShuvenduRoy/UnMixMatch"><i class="fab fa-fw fa-github-square" aria-hidden="true"></i> Code</a>-->
<!--                            <br><br>-->
<!--                            <p style="text-align: left; font-size: .90em;"  id="aaai24_tldr">-->
<!--                                <b> TLDR: </b> We introduce UnMixMatch, a semi-supervised learning framework designed to leverage unconstrained unlabeled data to enhance scalability and generalizability. Unlike existing methods that assume labeled and unlabeled data come from the same distribution, UnMixMatch overcomes this limitation with three key components: a supervised learner with strong regularization, a contrastive consistency regularizer, and a self-supervised loss.-->
<!--                            </p>-->
<!--                            <p style="text-align: left; font-size: .90em;" class="hide" id="aaai24">-->
<!--                                <b> Abstract: </b> We propose UnMixMatch, a semi-supervised learning framework which can learn effective representations from unconstrained unlabelled data in order to scale up performance. Most existing semi-supervised methods rely on the assumption that labelled and unlabelled samples are drawn from the same distribution, which limits the potential for improvement through the use of free-living unlabeled data. Consequently, the generalizability and scalability of semi-supervised learning are often hindered by this assumption. Our method aims to overcome these constraints and effectively utilize unconstrained unlabelled data in semi-supervised learning. UnMixMatch consists of three main components: a supervised learner with hard augmentations that provides strong regularization, a contrastive consistency regularizer to learn underlying representations from the unlabelled data, and a self-supervised loss to enhance the representations that are learnt from the unlabelled data. We perform extensive experiments on 4 commonly used datasets and demonstrate superior performance over existing semi-supervised methods with a performance boost of 4.79%. Extensive ablation and sensitivity studies show the effectiveness and impact of each of the proposed components of our method. The code for our work is publicly available.-->
<!--                            </p>-->
<!--                        </td>-->
<!--                    </tr>-->




                    <tr onmouseout="tmlr24_stop()" onmouseover="tmlr24_start()" bgcolor="#ffffff">
                        <td style="width:25%; vertical-align:center">
                            <div class="one">
                                <div class="two" id='tmlr24_image'>
                                    <img src="img/tmlr24_1.png" id="tmlr24_img_2" width="200">
                                </div>
                                <img src='img/tmlr24_1.png' id="tmlr24_img_1" width="200">
                            </div>
                            <script type="text/javascript">
                                function tmlr24_start() {
                                    document.getElementById('tmlr24_image').style.opacity = "1";
                                    document.getElementById('tmlr24').style.display = 'block';
                                    document.getElementById('tmlr24_img_1').style.opacity = "0";
                                    document.getElementById('tmlr24_tldr').style.display = 'none';

                                }

                                function tmlr24_stop() {
                                    document.getElementById('tmlr24_image').style.opacity = "0";
                                    document.getElementById('tmlr24').style.display = "none";
                                    document.getElementById('tmlr24_img_1').style.opacity = "1";
                                    document.getElementById('tmlr24_tldr').style.display = 'block';

                                }

                                tmlr24_stop()
                            </script>
                        </td>
                        <td style="width:75% ;vertical-align:top">
                            <a href="https://openreview.net/pdf?id=DiyYf1Kcdt" style="text-decoration: None">
                                <strong>A Bag of Tricks for Few-Shot Class-Incremental Learning</strong>
                            </a>
                            <br><br>
                            <strong>Shuvendu Roy</strong>, Chunjong Park, Aldi Fahrezi, Ali Etemad
                            <br>
                            Transactions on Machine Learning Research (<b>TMLR 2024</b>) <br>
                            <br>

                            <a style="text-decoration: None" href="https://ojs.aaai.org/index.php/AAAI/article/view/29404" target="_blank">Paper</a> |
                            <a style="text-decoration: None" href="https://arxiv.org/pdf/2403.14392" target="_blank">ArXiv</a>
                            <br><br>
                            <p style="text-align: left; font-size: .90em;"  id="tmlr24_tldr">
                                <b> TLDR: </b> We propose a unified bag-of-tricks framework for few-shot class-incremental learning (FSCIL), enhancing stability and adaptability to new tasks with limited samples. Organized into stability, adaptability, and training tricks, our approach mitigates forgetting, improves new class learning, and boosts overall performance.
                            </p>
                            <p style="text-align: left; font-size: .90em;" class="hide" id="tmlr24">
                                <b> Abstract: </b> We present a bag of tricks framework for few-shot class-incremental learning (FSCIL), which is
                                a challenging form of continual learning that involves continuous adaptation to new tasks with
                                limited samples. FSCIL requires both stability and adaptability, i.e., preserving proficiency
                                in previously learned tasks while learning new ones. Our proposed bag of tricks brings
                                together six key and highly influential techniques that improve stability, adaptability, and
                                overall performance under a unified framework for FSCIL. We organize these tricks into three
                                categories: stability tricks, adaptability tricks, and training tricks. Stability tricks aim to
                                mitigate the forgetting of previously learned classes by enhancing the separation between the
                                embeddings of learned classes and minimizing interference when learning new ones. On the
                                other hand, adaptability tricks focus on the effective learning of new classes. Finally, training
                                tricks improve the overall performance without compromising stability or adaptability. We
                                perform extensive experiments on three benchmark datasets, CIFAR-100, CUB-200, and
                                miniIMageNet, to evaluate the impact of our proposed framework. Our detailed analysis
                                shows that our approach substantially improves both stability and adaptability, establishing
                                a new state-of-the-art by outperforming prior works in the area. We believe our method
                                provides a go-to solution and establishes a robust baseline for future research in this area.
                            </p>
                        </td>
                    </tr>



                    <tr onmouseout="iclr24_stop()" onmouseover="iclr24_start()" bgcolor="#ffffff">
                        <td style="width:25%; vertical-align:center">
                            <div class="one">
                                <div class="two" id='iclr24_image'>
                                    <img src="img/iclr24_img_2.png" id="iclr24_img_2" width="200">
                                </div>
                                <img src='img/iclr24_img_1.png' id="iclr24_img_1" width="200">
                            </div>
                            <script type="text/javascript">
                                function iclr24_start() {
                                    document.getElementById('iclr24_image').style.opacity = "1";
                                    document.getElementById('iclr24').style.display = 'block';
                                    document.getElementById('iclr24_img_1').style.opacity = "0";
                                    document.getElementById('iclr24_tldr').style.display = 'none';

                                }

                                function iclr24_stop() {
                                    document.getElementById('iclr24_image').style.opacity = "0";
                                    document.getElementById('iclr24').style.display = "none";
                                    document.getElementById('iclr24_img_1').style.opacity = "1";
                                    document.getElementById('iclr24_tldr').style.display = 'block';

                                }

                                iclr24_stop()
                            </script>
                        </td>
                        <td style="width:75% ;vertical-align:top">
                            <a href="https://openreview.net/pdf?id=wsRXwlwx4w" style="text-decoration: None">
                                <strong>Consistency-guided Prompt Learning for Vision-Language Models</strong>
                            </a>
                            <br>
                            International Conference on Learning Representations (<b>ICLR 2024</b>)
                            <br>
                            <hr>
                            <i>Workshop version:</i>
                            <a href="https://openreview.net/pdf?id=r7YtqrcPQN" style="text-decoration: None">
                                <strong>Learning Through Consistency for Prompt Tuning</strong>
                            </a>
                            <br>
                            <b>NeurIPS 2023</b> Workshop on Robustness of Few-shot and Zero-shot Learning in Foundation Models (R0-FoMo).
                            <br>
                            <hr>

                            <strong>Shuvendu Roy</strong>, Ali Etemad <br>

                            <a style="text-decoration: None" href="https://openreview.net/pdf?id=wsRXwlwx4w" target="_blank">Paper</a> |
                            <a style="text-decoration: None" href="https://arxiv.org/pdf/2306.01195" target="_blank">ArXiv</a> |
                            <a style="text-decoration: None" href="https://openreview.net/pdf?id=r7YtqrcPQN" target="_blank">Workshop Paper</a> |
                            <a style="text-decoration: None" href="https://github.com/ShuvenduRoy/CoPrompt"><i class="fab fa-fw fa-github-square" aria-hidden="true"></i> Code</a>
                            <br><br>
                            <p style="text-align: left; font-size: .90em;"  id="iclr24_tldr">
                                <b> TLDR: </b> We propose Consistency-guided Prompt learning (CoPrompt), a fine-tuning method for vision-language models that enhances generalization in few-shot settings. CoPrompt prevents overfitting by enforcing a consistency constraint between the trainable and pre-trained models, incorporating perturbed inputs for regularization, and combining prompts with adapters for greater tuning flexibility.
                            </p>
                            <p style="text-align: left; font-size: .90em;" class="hide" id="iclr24">
                                <b> Abstract: </b> We propose Consistency-guided Prompt learning (CoPrompt), a new fine-tuning method for vision-language models. Our approach improves the generalization of large foundation models when fine-tuned on downstream tasks in a few-shot setting. The basic idea of CoPrompt is to enforce a consistency constraint in the prediction of the trainable and pre-trained models to prevent overfitting on the downstream task. Additionally, we introduce the following two components into our consistency constraint to further boost the performance: enforcing consistency on two perturbed inputs and combining two dominant paradigms of tuning, prompting and adapter. Enforcing consistency on perturbed input serves to further regularize the consistency constraint, thereby improving generalization. Moreover, the integration of adapters and prompts not only enhances performance on downstream tasks but also offers increased tuning flexibility in both input and output spaces. This facilitates more effective adaptation to downstream tasks in a few-shot learning setting. Experiments show that CoPrompt outperforms existing methods on a range of evaluation suites, including base-to-novel generalization, domain generalization, and cross-dataset evaluation. On generalization, CoPrompt improves the state-of-the-art on zero-shot tasks and the overall harmonic mean over 11 datasets. Detailed ablation studies show the effectiveness of each of the components in CoPrompt. We make our code available at https://github.com/ShuvenduRoy/CoPrompt.
                            </p>
                        </td>
                    </tr>


                    <tr onmouseout="aaai24_stop()" onmouseover="aaai24_start()" bgcolor="#ffffff">
                        <td style="width:25%; vertical-align:center">
                            <div class="one">
                                <div class="two" id='aaai24_image'>
                                    <img src="img/aaai24_2.png" id="aaai24_img_2" width="200">
                                </div>
                                <img src='img/aaai24_1.png' id="aaai24_img_1" width="200">
                            </div>
                            <script type="text/javascript">
                                function aaai24_start() {
                                    document.getElementById('aaai24_image').style.opacity = "1";
                                    document.getElementById('aaai24').style.display = 'block';
                                    document.getElementById('aaai24_img_1').style.opacity = "0";
                                    document.getElementById('aaai24_tldr').style.display = 'none';

                                }

                                function aaai24_stop() {
                                    document.getElementById('aaai24_image').style.opacity = "0";
                                    document.getElementById('aaai24').style.display = "none";
                                    document.getElementById('aaai24_img_1').style.opacity = "1";
                                    document.getElementById('aaai24_tldr').style.display = 'block';

                                }

                                aaai24_stop()
                            </script>
                        </td>
                        <td style="width:75% ;vertical-align:top">
                            <a href="https://ojs.aaai.org/index.php/AAAI/article/view/29404" style="text-decoration: None">
                                <strong>Scaling Up Semi-supervised Learning with Unconstrained Unlabelled Data</strong>
                            </a>
                            <br>
                            AAAI Conference on Artificial Intelligence (<b>AAAI 2024</b>)
                            <br>
                            <hr>
                            <i>Workshop version:</i>
                            <a href="https://sslneurips23.github.io/paper_pdfs/paper_7.pdf" style="text-decoration: None">
                                <strong>Does Unconstrained Unlabeled Data Help Semi-Supervised Learning?</strong>
                            </a>
                            <br>
                            <b>NeurIPS 2023</b> Workshop: Self-Supervised Learning - Theory and Practice.
                            <br>
                            <hr>

                            <strong>Shuvendu Roy</strong>, Ali Etemad <br>

                            <a style="text-decoration: None" href="https://ojs.aaai.org/index.php/AAAI/article/view/29404" target="_blank">Paper</a> |
                            <a style="text-decoration: None" href="https://arxiv.org/pdf/2306.01222" target="_blank">ArXiv</a> |
                            <a style="text-decoration: None" href="https://sslneurips23.github.io/paper_pdfs/paper_7.pdf" target="_blank">Workshop Paper</a> |
                            <a style="text-decoration: None" href="https://github.com/ShuvenduRoy/UnMixMatch"><i class="fab fa-fw fa-github-square" aria-hidden="true"></i> Code</a>
                            <br><br>
                            <p style="text-align: left; font-size: .90em;"  id="aaai24_tldr">
                                <b> TLDR: </b> We introduce UnMixMatch, a semi-supervised learning framework designed to leverage unconstrained unlabeled data to enhance scalability and generalizability. Unlike existing methods that assume labeled and unlabeled data come from the same distribution, UnMixMatch overcomes this limitation with three key components: a supervised learner with strong regularization, a contrastive consistency regularizer, and a self-supervised loss.
                            </p>
                            <p style="text-align: left; font-size: .90em;" class="hide" id="aaai24">
                                <b> Abstract: </b> We propose UnMixMatch, a semi-supervised learning framework which can learn effective representations from unconstrained unlabelled data in order to scale up performance. Most existing semi-supervised methods rely on the assumption that labelled and unlabelled samples are drawn from the same distribution, which limits the potential for improvement through the use of free-living unlabeled data. Consequently, the generalizability and scalability of semi-supervised learning are often hindered by this assumption. Our method aims to overcome these constraints and effectively utilize unconstrained unlabelled data in semi-supervised learning. UnMixMatch consists of three main components: a supervised learner with hard augmentations that provides strong regularization, a contrastive consistency regularizer to learn underlying representations from the unlabelled data, and a self-supervised loss to enhance the representations that are learnt from the unlabelled data. We perform extensive experiments on 4 commonly used datasets and demonstrate superior performance over existing semi-supervised methods with a performance boost of 4.79%. Extensive ablation and sensitivity studies show the effectiveness and impact of each of the proposed components of our method. The code for our work is publicly available.
                            </p>
                        </td>
                    </tr>


                    <tr onmouseout="tetci24_stop()" onmouseover="tetci24_start()" bgcolor="#ffffff">
                        <td style="width:25%; vertical-align:center">
                            <div class="one">
                                <div class="two" id='tetci24_image'>
                                    <img src="img/tetci24_1.png" id="tetci24_img_2" width="200">
                                </div>
                                <img src='img/tetci24_1.png' id="tetci24_img_1" width="200">
                            </div>
                            <script type="text/javascript">
                                function tetci24_start() {
                                    document.getElementById('tetci24_image').style.opacity = "1";
                                    document.getElementById('tetci24').style.display = 'block';
                                    document.getElementById('tetci24_img_1').style.opacity = "0";
                                    document.getElementById('tetci24_tldr').style.display = 'none';

                                }

                                function tetci24_stop() {
                                    document.getElementById('tetci24_image').style.opacity = "0";
                                    document.getElementById('tetci24').style.display = "none";
                                    document.getElementById('tetci24_img_1').style.opacity = "1";
                                    document.getElementById('tetci24_tldr').style.display = 'block';

                                }

                                tetci24_stop()
                            </script>
                        </td>
                        <td style="width:75% ;vertical-align:top">
                            <a href="https://arxiv.org/pdf/2211.14912" style="text-decoration: None">
                                <strong>Impact of Strategic Sampling and Supervision Policies on Semi-supervised Learning</strong>
                            </a>
                            <br><br>
                            <strong>Shuvendu Roy</strong>, Ali Etemad
                            <br>
                            IEEE Transactions on Emerging Topics in Computational Intelligence (<b>IEEE TETCI 2024</b>) <br>
                            <br>

                            <a style="text-decoration: None" href="https://arxiv.org/abs/2211.14912" target="_blank">Paper</a> |
                            <a style="text-decoration: None" href="https://arxiv.org/pdf/2211.14912" target="_blank">ArXiv</a>
                            <br><br>
                            <p style="text-align: left; font-size: .90em;"  id="tetci24_tldr">
                                <b> TLDR: </b> This study investigates the impact of labeled sample selection and usage in semi-supervised learning when labeled data is scarce. Selecting representative samples for labeling improves performance by up to 7.5% in low-label scenarios, while label injection strategies during training show minimal effect.
                            </p>
                            <p style="text-align: left; font-size: .90em;" class="hide" id="tetci24">
                                <b> Abstract: </b> In semi-supervised representation learning frameworks, when the number of labelled data is very scarce, the
                                quality and representativeness of these samples become increasingly important. Existing literature on semi-supervised learning
                                randomly sample a limited number of data points for labelling.
                                All these labelled samples are then used along with the unlabelled
                                data throughout the training process. In this work, we ask
                                two important questions in this context: (1) does it matter
                                which samples are selected for labelling? (2) does it matter how
                                the labelled samples are used throughout the training process
                                along with the unlabelled data? To answer the first question,
                                we explore a number of unsupervised methods for selecting
                                specific subsets of data to label (without prior knowledge of their
                                labels), with the goal of maximizing representativeness w.r.t. the
                                unlabelled set. Then, for our second line of inquiry, we define
                                a variety of different label injection strategies in the training
                                process. Extensive experiments on four popular datasets, CIFAR-
                                10, CIFAR-100, SVHN, and STL-10, show that unsupervised
                                selection of samples that are more representative of the entire
                                data improves performance by up to∼2% over the existing
                                semi-supervised frameworks such as MixMatch, ReMixMatch,
                                FixMatch and others with random sample labelling. We show
                                that this boost could even increase to 7.5% for very few-labelled
                                scenarios. However, our study shows that gradually injecting the
                                labels throughout the training procedure does not impact the
                                performance considerably versus when all the existing labels are
                                used throughout the entire training.
                            </p>
                        </td>
                    </tr>


                    <tr onmouseout="taffc24_stop()" onmouseover="taffc24_start()" bgcolor="#ffffff">
                        <td style="width:25%; vertical-align:center">
                            <div class="one">
                                <div class="two" id='taffc24_image'>
                                    <img src="img/taffc24_2.png" id="taffc24_img_2" width="200">
                                </div>
                                <img src='img/taffc24_1.png' id="taffc24_img_1" width="200">
                            </div>
                            <script type="text/javascript">
                                function taffc24_start() {
                                    document.getElementById('taffc24_image').style.opacity = "1";
                                    document.getElementById('taffc24').style.display = 'block';
                                    document.getElementById('taffc24_img_1').style.opacity = "0";
                                    document.getElementById('taffc24_tldr').style.display = 'none';

                                }

                                function taffc24_stop() {
                                    document.getElementById('taffc24_image').style.opacity = "0";
                                    document.getElementById('taffc24').style.display = "none";
                                    document.getElementById('taffc24_img_1').style.opacity = "1";
                                    document.getElementById('taffc24_tldr').style.display = 'block';

                                }

                                taffc24_stop()
                            </script>
                        </td>
                        <td style="width:75% ;vertical-align:top">
                            <a href="https://arxiv.org/pdf/2306.01229" style="text-decoration: None">
                                <strong>Exploring the Boundaries of Semi-Supervised Facial Expression Recognition using In-Distribution, Out-of-Distribution, and Unconstrained Data</strong>
                            </a>
                            <br><br>
                            <strong>Shuvendu Roy</strong>, Ali Etemad
                            <br>
                            IEEE Transactions on Affective Computing (<b>IEEE TAFFC 2024</b>) <br>
                            <br>

                            <a style="text-decoration: None" href="https://ieeexplore.ieee.org/abstract/document/10591411" target="_blank">Paper</a> |
                            <a style="text-decoration: None" href="https://arxiv.org/pdf/2306.01229" target="_blank">ArXiv</a> |
                            <a style="text-decoration: None" href="https://github.com/ShuvenduRoy/SSL_FER_OOD"><i class="fab fa-fw fa-github-square" aria-hidden="true"></i> Code</a>
                            <br><br>
                            <p style="text-align: left; font-size: .90em;"  id="taffc24_tldr">
                                <b> TLDR: </b> This study evaluates 11 recent semi-supervised learning methods for facial expression recognition (FER) across diverse settings, including in-distribution, out-of-distribution, and unconstrained data. FixMatch excels with in-distribution data, while ReMixMatch performs best in challenging scenarios, showcasing the consistent benefits of semi-supervised learning over supervised methods.
                            </p>
                            <p style="text-align: left; font-size: .90em;" class="hide" id="taffc24">
                                <b> Abstract: </b> Deep learning-based methods have been the key driving force behind much of the recent success of facial expression
                                recognition (FER) systems. However, the need for large amounts of labelled data remains a challenge. Semi-supervised learning offers
                                a way to overcome this limitation, allowing models to learn from a small amount of labelled data along with a large unlabelled dataset.
                                While semi-supervised learning has shown promise in FER, most current methods from general computer vision literature have not
                                been explored in the context of FER. In this work, we present a comprehensive study on 11 of the most recent semi-supervised
                                methods, in the context of FER, namely Pi-model, Pseudo-label, Mean Teacher, VAT, UDA, MixMatch, ReMixMatch, FlexMatch,
                                CoMatch, and CCSSL. Our investigation covers semi-supervised learning from in-distribution, out-of-distribution, unconstrained, and
                                very small unlabelled data. Our evaluation includes five FER datasets plus one large face dataset for unconstrained learning. Our
                                results demonstrate that FixMatch consistently achieves better performance on in-distribution unlabelled data, while ReMixMatch
                                stands out among all methods for out-of-distribution, unconstrained, and scarce unlabelled data scenarios. Another significant
                                observation is that semi-supervised learning produces a reasonable improvement over supervised learning, regardless of whether
                                in-distribution, out-of-distribution, or unconstrained data is utilized as the unlabelled set. We also conduct sensitivity analyses on critical
                                hyper-parameters for the two best methods of each setting.
                            </p>
                        </td>
                    </tr>


                    <tr onmouseout="tomm23_stop()" onmouseover="tomm23_start()" bgcolor="#ffffff">
                        <td style="width:25%; vertical-align:center">
                            <div class="one">
                                <div class="two" id='tomm23_image'>
                                    <img src="img/tomm23.png" width="200">
                                </div>
                                <img src='img/tomm23.png' width="200">
                            </div>
                            <script type="text/javascript">
                                function tomm23_start() {
                                    document.getElementById('tomm23_image').style.opacity = "1";
                                    document.getElementById('tomm23').style.display = 'block';
                                    document.getElementById('tomm23_tldr').style.display = 'none';
                                }

                                function tomm23_stop() {
                                    document.getElementById('tomm23_image').style.opacity = "0";
                                    document.getElementById('tomm23').style.display = "none";
                                    document.getElementById('tomm23_tldr').style.display = "block";
                                }

                                tomm23_stop()
                            </script>
                        </td>
                        <td style="width:75% ;vertical-align:top">
                            <a href="https://arxiv.org/pdf/2311.06852" style="text-decoration: None">
                                <strong>Contrastive Learning of View-invariant Representations for Facial Expressions Recognition</strong>
                            </a>
                            <br><br>
                            <strong>Shuvendu Roy</strong>, Ali Etemad
                            <br>
                            ACM Transactions on Multimedia Computing, Communications and Applications (<b>ACM TOMM 2023</b>) <br>
                            <br>

                            <a style="text-decoration: None" href="https://dl.acm.org/doi/abs/10.1145/3632960">Paper</a> |
                            <a style="text-decoration: None" href="https://arxiv.org/pdf/2311.06852">ArVix</a>
                            <br><br>
                            <p style="text-align: left; font-size: .90em;"  id="tomm23_tldr">
                                <b> TLDR: </b> Introducing ViewFX, a novel facial expression recognition framework that accurately classifies emotions from various angles. ViewFX uses contrastive learning to learn view-invariant representation of the expression, and enable understanding expression from any view angles.
                            </p>
                            <p style="text-align: left; font-size: .90em;" class="hide" id="tomm23">
                                <b> Abstract: </b>Although there has been much progress in the area of facial expression recognition (FER), most existing methods suffer when presented with images that have been captured from viewing angles that are non-frontal and substantially different from those used in the training process. In this article, we propose ViewFX, a novel view-invariant FER framework based on contrastive learning, capable of accurately classifying facial expressions regardless of the input viewing angles during inference. ViewFX learns view-invariant features of expression using a proposed self-supervised contrastive loss, which brings together different views of the same subject with a particular expression in the embedding space. We also introduce a supervised contrastive loss to push the learned view-invariant features of each expression away from other expressions. Since facial expressions are often distinguished with very subtle differences in the learned feature space, we incorporate the Barlow twins loss to reduce the redundancy and correlations of the representations in the learned representations. The proposed method is a substantial extension of our previously proposed CL-MEx, which only had a self-supervised loss. We test the proposed framework on two public multi-view facial expression recognition datasets, KDEF and DDCF. The experiments demonstrate that our approach outperforms previous works in the area and sets a new state-of-the-art for both datasets while showing considerably less sensitivity to challenging angles and the number of output labels used for training. We also perform detailed sensitivity and ablation experiments to evaluate the impact of different components of our model as well as its sensitivity to different parameters.
                            </p>
                        </td>
                    </tr>


                    <tr onmouseout="acii23_stop()" onmouseover="acii23_start()" bgcolor="#ffffff">
                        <td style="width:25%; vertical-align:center">
                            <div class="one">
                                <div class="two" id='acii23_image'>
                                    <img src="img/acii23.png" width="200">
                                </div>
                                <img src='img/acii23.png' width="200">
                            </div>
                            <script type="text/javascript">
                                function acii23_start() {
                                    document.getElementById('acii23_image').style.opacity = "1";
                                    document.getElementById('acii23').style.display = 'block';
                                }

                                function acii23_stop() {
                                    document.getElementById('acii23_image').style.opacity = "0";
                                    document.getElementById('acii23').style.display = "none";
                                }

                                acii23_stop()
                            </script>
                        </td>
                        <td style="width:75% ;vertical-align:top">
                            <a href="https://arxiv.org/abs/2307.02744" style="text-decoration: None">
                                <strong>Active Learning with Contrastive Pre-training for Facial Expression Recognition</strong>
                            </a>
                            <br><br>
                            <strong>Shuvendu Roy</strong>, Ali Etemad
                            <br>
                            11th International Conference on Affective Computing and Intelligent Interaction (<b>ACII 2023</b>) <br>
                            <br>

                            <a style="text-decoration: None" href="https://ieeexplore.ieee.org/abstract/document/10388093">Paper</a> |
                            <a style="text-decoration: None" href="https://arxiv.org/pdf/2307.02744">ArVix</a>  |
                            <a style="text-decoration: None" href="https://github.com/ShuvenduRoy/ActiveFER"><i class="fab fa-fw fa-github-square" aria-hidden="true"></i> Code</a>
                            <br><br>
                            <p style="text-align: left; font-size: .90em;" class="hide" id="acii23">
                                Deep learning has played a significant role in the
                                success of facial expression recognition (FER), thanks to large
                                models and vast amounts of labelled data. However, obtaining
                                labelled data requires a tremendous amount of human effort,
                                time, and financial resources. Even though some prior works
                                have focused on reducing the need for large amounts of labelled
                                data using different unsupervised methods, another promising
                                approach called active learning is barely explored in the context
                                of FER. This approach involves selecting and labelling the most
                                representative samples from an unlabelled set to make the best
                                use of a limited ‘labelling budget’. In this paper, we implement
                                and study 8 recent active learning methods on three public
                                FER datasets, FER13, RAF-DB, and KDEF. Our findings show
                                that existing active learning methods do not perform well in
                                the context of FER, likely suffering from a phenomenon called
                                ‘Cold Start’, which occurs when the initial set of labelled samples
                                is not well representative of the entire dataset. To address this
                                issue, we propose contrastive self-supervised pre-training, which
                                first learns the underlying representations based on the entire
                                unlabelled dataset. We then follow this with the active learning
                                methods and observe that our 2-step approach shows up to 9.2%
                                improvement over random sampling and up to 6.7% improvement over the best existing active learning baseline without the
                                pre-training. We will make the code for this study public upon
                                publication at: github.com/ShuvenduRoy/ActiveFER.
                            </p>
                            <br>
                        </td>
                    </tr>


                    <tr onmouseout="icassp23_stop()" onmouseover="icassp23_start()" bgcolor="#ffffff">
                        <td style="width:25%; vertical-align:center">
                            <div class="one">
                                <div class="two" id='icassp23_image'>
                                    <img src="img/icassp23.png" width="200">
                                </div>
                                <img src='img/icassp23.png' width="200">
                            </div>
                            <script type="text/javascript">
                                function icassp23_start() {
                                    document.getElementById('icassp23_image').style.opacity = "1";
                                    document.getElementById('icassp23').style.display = 'block';
                                }

                                function icassp23_stop() {
                                    document.getElementById('icassp23_image').style.opacity = "0";
                                    document.getElementById('icassp23').style.display = "none";
                                }

                                icassp23_stop()
                            </script>
                        </td>
                        <td style="width:75% ;vertical-align:top">
                            <a href="https://arxiv.org/abs/2209.00760" style="text-decoration: None">
                                <strong>Temporal Contrastive Learning with Curriculum</strong>
                            </a>
                            <br><br>
                            <strong>Shuvendu Roy</strong>, Ali Etemad
                            <br>
                            IEEE International Conference on Acoustics, Speech and Signal Processing, (<b>ICASSP 2023</b>) <br>
                            <br>

                            <a style="text-decoration: None" href="https://ieeexplore.ieee.org/abstract/document/10096811">Paper</a> |
                            <a style="text-decoration: None" href="https://arxiv.org/pdf/2209.00760">ArVix</a>
                            <br><br>
                            <p style="text-align: left; font-size: .90em;" class="hide" id="icassp23">
                                We present ConCur, a contrastive video representation learning method that uses curriculum learning
                                to impose a dynamic sampling strategy in contrastive training. More specifically, ConCur starts the
                                contrastive training with easy positive samples (temporally close and semantically similar clips),
                                and as the training progresses, it increases the temporal span effectively sampling hard positives
                                (temporally away and semantically dissimilar). To learn better context-aware representations, we also
                                propose an auxiliary task of predicting the temporal distance between a positive pair of clips. We
                                conduct extensive experiments on two popular action recognition datasets, UCF101 and HMDB51, on
                                which our proposed method achieves state-of-the-art performance on two benchmark tasks of video
                                action recognition and video retrieval. We explore the impact of encoder backbones and pre-training
                                strategies by using R(2+1)D and C3D encoders and pre-training on Kinetics-400 and Kinetics-200
                                datasets. Moreover, a detailed ablation study shows the effectiveness of each of the components of
                                our proposed method.
                            </p>
                            <br>
                        </td>
                    </tr>



                    <tr onmouseout="acii22_stop()" onmouseover="acii22_start()" bgcolor="#ffffff">
                        <td style="width:25%; vertical-align:center">
                            <div class="one">
                                <div class="two" id='acii22_image'>
                                    <img src="img/acii22.png" width="200">
                                </div>
                                <img src='img/acii22.png' width="200">
                            </div>
                            <script type="text/javascript">
                                function acii22_start() {
                                    document.getElementById('acii22_image').style.opacity = "1";
                                    document.getElementById('acii2022').style.display = 'block';
                                }

                                function acii22_stop() {
                                    document.getElementById('acii22_image').style.opacity = "0";
                                    document.getElementById('acii2022').style.display = "none";
                                }

                                acii22_stop()
                            </script>
                        </td>
                        <td style="width:75% ;vertical-align:top">
                            <a href="https://arxiv.org/abs/2208.00544" style="text-decoration: None">
                                <strong>Analysis of Semi-Supervised Methods for Facial Expression Recognition</strong>
                            </a>
                            <br><br>
                            <strong>Shuvendu Roy</strong>, Ali Etemad
                            <br>
                            IEEE International Conference on Affective Computing and Intelligent Interaction <b>(ACII 2022)</b> <br>
                            <br>

                            <a style="text-decoration: None" href="https://arxiv.org/pdf/2208.00544.pdf">Paper</a> |
                            <a style="text-decoration: None" href="https://shuvenduroy.github.io/SSL_FER/" target="_blank">Project Page</a> |
                            <a style="text-decoration: None" href="https://github.com/ShuvenduRoy/SSL_FER"><i class="fab fa-fw fa-github-square" aria-hidden="true"></i> Code</a>

                            <br><br>
                            <p style="text-align: left; font-size: .90em;" class="hide" id="acii2022">
                                Training deep neural networks for image recognition often requires large-scale human annotated data. To reduce the reliance of deep neural solutions on labeled data, state-of-the-art semi-supervised methods have been proposed in the literature. Nonetheless, the use of such semi-supervised methods has been quite rare in the field of facial expression recognition (FER). In this paper, we present a comprehensive study on recently proposed state-of-the-art semi-supervised learning methods in the context of FER. We conduct comparative study on eight semi-supervised learning methods, namely Pi-Model, Pseudo-label, Mean-Teacher, VAT, MixMatch, ReMixMatch, UDA, and FixMatch, on three FER datasets (FER13, RAF-DB, and AffectNet), when various amounts of labeled samples are used. We also compare the performance of these methods against fully-supervised training. Our study shows that when training existing semi-supervised methods on as little as 250 labeled samples per class can yield comparable performances to that of fully-supervised methods trained on the full labeled datasets. To facilitate further research in this area, we make our code publicly available at: <a style="text-decoration: None" href="https://github.com/ShuvenduRoy/SSL_FER">https://github.com/ShuvenduRoy/SSL_FER</a>.
                            </p>
                            <br>
                        </td>
                    </tr>


                    <tr onmouseout="icmi_stop()" onmouseover="icmi_start()" bgcolor="#ffffff">
                        <td style="width:25%; vertical-align:center">
                            <div class="one">
                                <div class="two" id='icmi_image'>
                                    <img src="img/CLMEX2.png" width="200">
                                </div>
                                <img src='img/CLMEX.png' width="200">
                            </div>
                            <script type="text/javascript">
                                function icmi_start() {
                                    document.getElementById('icmi_image').style.opacity = "1";
                                    document.getElementById('icmi2021').style.display = 'block';
                                }

                                function icmi_stop() {
                                    document.getElementById('icmi_image').style.opacity = "0";
                                    document.getElementById('icmi2021').style.display = "none";
                                }

                                icmi_stop()
                            </script>
                        </td>
                        <td style="width:75% ;vertical-align:top">
                            <a href="https://dl.acm.org/doi/abs/10.1145/3462244.3479955" style="text-decoration: None">
                                <strong>Self-supervised Contrastive Learning of Multi-view Facial Expressions</strong>
                            </a>
                            <br><br>
                            <strong>Shuvendu Roy</strong>, Ali Etemad
                            <br>
                            ACM International Conference on Multimodal Interaction <b>(ICMI 2021)</b> <br>
                            <br>

                            <a style="text-decoration: None" href="https://dl.acm.org/doi/abs/10.1145/3462244.3479955" target="_blank">Paper</a> |
                            <a style="text-decoration: None" href="https://arxiv.org/abs/2108.06723" target="_blank">ArXiv</a>
                            <br><br>
                            <p style="text-align: left; font-size: .90em;" class="hide" id="icmi2021">We propose Contrastive Learning of Multi-view facial Expressions (CL-MEx) to exploit facial images captured simultaneously from different angles towards FER. CL-MEx is a two-step training framework. In the first step, an encoder network is pre-trained with the proposed self-supervised contrastive loss, where it learns to generate view-invariant embeddings for different views of a subject. The model is then fine-tuned with labeled data in a supervised setting. We demonstrate the performance of the proposed method on two multi-view FER datasets, KDEF and DDCF, where state-of-the-art performances are achieved. Further experiments show the robustness of our method in dealing with challenging angles and reduced amounts of labeled data.</p>
                            <br>
                        </td>
                    </tr>


                    <tr onmouseout="dense_stop()" onmouseover="dense_start()" bgcolor="#ffffff">
                        <td style="width:25%; vertical-align:center">
                            <div class="one">
                                <div class="two" id='acii_image'>
                                    <img src="img/SLCLR.png" width="200">
                                </div>
                                <img src='img/SLCLR2.png' width="200">
                            </div>
                            <script type="text/javascript">
                                function dense_start() {
                                    document.getElementById('acii_image').style.opacity = "1";
                                    document.getElementById('acii_2021').style.display = 'block';
                                }

                                function dense_stop() {
                                    document.getElementById('acii_image').style.opacity = "0";
                                    document.getElementById('acii_2021').style.display = "none";
                                }

                                dense_stop()
                            </script>
                        </td>

                        <td style="width:75% ;vertical-align:top">
                            <a href="https://ieeexplore.ieee.org/abstract/document/9597460" style="text-decoration: None">
                                <strong>Spatiotemporal Contrastive Learning of Facial Expressions in Videos</strong>
                            </a>
                            <br><br>
                            <strong>Shuvendu Roy</strong>, Ali Etemad
                            <br>
                            IEEE International Conference on Affective Computing and Intelligent Interaction (<b>ACII 2021</b>) <br>
                            <br>

                            <a style="text-decoration: None" href="https://ieeexplore.ieee.org/abstract/document/9597460" target="_blank">Paper</a> |
                            <a style="text-decoration: None" href="https://arxiv.org/abs/2108.03064" target="_blank">ArXiv</a>
                            <br> <br>

                            <p style="text-align: left; font-size: .90em;" class="hide" id="acii_2021">We propose a self-supervised contrastive learning approach for facial expression recognition (FER) in videos. We propose a novel temporal sampling-based augmentation scheme to be utilized in addition to standard spatial augmentations used for contrastive learning. Our proposed temporal augmentation scheme randomly picks from one of three temporal sampling techniques: (1) pure random sampling, (2) uniform sampling, and (3) sequential sampling. This is followed by a combination of up to three standard spatial augmentations. We then use a deep R(2+1)D network for FER, which we train in a self-supervised fashion based on the augmentations and subsequently fine-tune.</p>
                            <br>
                        </td>
                    </tr>


                    <tr onmouseout="thesis_stop()" onmouseover="thesis_start()" bgcolor="#ffffff">
                        <td style="width:25%; vertical-align:center">
                            <div class="one">
                                <div class="two" id='thesis_image'>
                                    <img src="img/tl.png" width="200">
                                </div>
                                <img src='img/tl.png' width="200">
                            </div>
                            <script type="text/javascript">
                                function thesis_start() {
                                    document.getElementById('thesis_image').style.opacity = "1";
                                    document.getElementById('thesis').style.display = 'block';
                                }

                                function thesis_stop() {
                                    document.getElementById('thesis_image').style.opacity = "0";
                                    document.getElementById('thesis').style.display = "none";
                                }

                                thesis_stop()
                            </script>
                        </td>

                        <td style="width:75% ;vertical-align:top">
                            <a href="https://dl.acm.org/doi/abs/10.1145/3462244.3479955" style="text-decoration: None">
                                <strong>Facial Emotion Recognition Using Transfer Learning in the Deep CNN</strong>
                            </a>
                            <br><br>
                            MAH Akhand, <strong>Shuvendu Roy</strong>, Nazmul Siddique, Md Abdus Samad Kamal, Tetsuya Shimamura
                            <br>
                            Electronics 10 (9), 2021 <br>
                            <br>

                            <a style="text-decoration: None" href="https://www.mdpi.com/2079-9292/10/9/1036" target="_blank">Paper</a> |
                            <a style="text-decoration: None" href="" target="_blank">Undergrad Thesis</a> |
                            <a style="text-decoration: None" href="https://github.com/ShuvenduRoy/FER_TL_PipelineTraining"><i class="fab fa-fw fa-github-square" aria-hidden="true"></i> Code</a> |
                            <a style="text-decoration: None" href="https://github.com/EverLookNeverSee/fer_tl_dcnn"><i class="fab fa-fw fa-github-square" aria-hidden="true"></i> Third Party Implementation</a>
                            <br>

                            <p style="text-align: left; font-size: .90em;" class="hide" id="thesis">For developing a highly accurate FER system, this study proposes a very Deep CNN (DCNN) modeling through Transfer Learning (TL) technique where a pre-trained DCNN model is adopted by replacing its dense upper layer(s) compatible with FER, and the model is fine-tuned with facial emotion data. A novel pipeline strategy is introduced, where the training of the dense layer(s) is followed by tuning each of the pre-trained DCNN blocks successively that has led to gradual improvement of the accuracy of FER to a higher level.</p>
                            <br>
                        </td>
                    </tr>


                    </tbody>
                </table>

                <br><br>

            </section>

<!--                <h2 dir="ltr" class="CDt4Ke zfr3Q JYVBee"><span style="color: #000000; font-family: 'Nunito'; font-size: 1.3em; font-weight: normal;">Academic Services</span><span-->
<!--                        style="color: #000000; font-family: 'Nunito'; font-size: 14pt; font-weight: normal; vertical-align: baseline;"></span></h2>-->
                <header><h1 class="page__title" itemprop="headline"><i>Academic Services</i></h1></header>
                <h3 style="margin-top: -5px; font-size: 1.1em; margin-left: 10px">Reviewing</h3>
<!--                <p dir="ltr" style="line-height: 1.8; text-align: left;">-->
<!--                        <a href="https://publons.com/researcher/4948440/shuvendu-roy/">Publons Profile</a>-->
<!--                </p>-->
                <ol class="three">
                    <li>Computer Vision and Pattern Recognition (CVPR), 2023, 2024</li>
                    <li>European Conference on Computer Vision (ECCV), 2022, 20024</li>
                    <li>European Conference on Learning Representations (ICLR), 2025</li>
                    <li>AAAI Conference on Artificial Intelligence, 2023, 2024, 2025</li>
                    <li>IEEE Transactions on Affective Computing (TAFFC) </li>
                    <li>IEEE Transactions on Artificial Intelligence (TAI) </li>
                    <li>AAAI'22 Workshop on Human-Centric Self-Supervised Learning </li>
                    <li>International Conference on Pattern Recognition (ICPR), 2022</li>
                    <li>International Journal of Electrical and Computer Engineering (IJECE)</li>
                    <li>Imaging Science Journal</li>
                    <li>PeerJ Computer Science</li>
                    <li>Springer Nature Artificial Intelligence Review, 2022</li>
<!--                    <li><a href="https://publons.com/researcher/4948440/shuvendu-roy/">Publons Profile</a></li>-->
                    <li><a href="https://www.webofscience.com/wos/author/record/3373612">Publons Profile</a></li>
                </ol>
                <hr>

                    <br><br>
                <p align="justify">



                <header><h1 class="page__title" itemprop="headline"><i>Recent Updates!</i></h1></header>

<!--                <h2 style="margin-bottom: -0.5em">Recent Updates!</h2>-->
                <p align="justify">
                <ul class="my_list">
                    <li> [Jan 2024] Starting working at Vector Institute as an Applied Machine Learning Intern</li>
                    <li> [May 2023] One paper accepted in TMLR'23 </li>
                    <li> [Feb 2023] Our paper CoPrompt is accepted in ICLR'23 </li>
                    <li> [Dec 2023] Our paper UnMixMatch is accepted in AAAI'24 </li>
                    <li> [May 2023] Starting working at Google as a Student Researcher</li>
                    <li> [Feb 2023] One paper accepted in ICASSP'23 </li>
                    <li> [Jul 2022] One paper accepted in ACII'22 </li>
                    <li> [Jan 2022] Started Ph.D. at Queen's University, Kingston, Canada </li>
                    <li> [Aug 2021] One paper accepted at ICMI'21</li>
                </ul>
                </p>


            </section>
            <footer class="page__meta"></footer>
        </div>
    </article>
</div>

<script src="backend/main.js"></script>
<script> (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r;
    i[r] = i[r] || function () {
        (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date();
    a = s.createElement(o), m = s.getElementsByTagName(o)[0];
    a.async = 1;
    a.src = g;
    m.parentNode.insertBefore(a, m)
})(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
ga('create', '', 'auto');
ga('send', 'pageview'); </script>
</body>
<grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration>
</html>